{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFN_DUDL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ANN"
      ],
      "metadata": {
        "id": "w6QcgftEKF2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFM9yNcVkWuT"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import small mnist dataset\n",
        "\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')"
      ],
      "metadata": {
        "id": "unAFVJUGk5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "XgIKHTuUlN0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[1,0]"
      ],
      "metadata": {
        "id": "45t0zVRTlTTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data[:,0]\n",
        "data = data[:,1:]"
      ],
      "metadata": {
        "id": "RY-N79N5lVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "7trjvaFUp2y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape, labels.shape"
      ],
      "metadata": {
        "id": "yfFp1jkJlqPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.imshow(np.reshape(data[0], (28,28)), cmap='gray')"
      ],
      "metadata": {
        "id": "HOduIUX7mxaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a few random digits\n",
        "fig,axs = plt.subplots(3,4,figsize=(10,6))\n",
        "\n",
        "for ax in axs.flatten():\n",
        "  # pick a random image\n",
        "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
        "\n",
        "  # create the image (must be reshaped!)\n",
        "  img = np.reshape(data[randimg2show,:],(28,28))\n",
        "  ax.imshow(img,cmap='gray')\n",
        "\n",
        "  # title\n",
        "  ax.set_title('The number %i'%labels[randimg2show])\n",
        "\n",
        "plt.suptitle('How humans see the data',fontsize=20)\n",
        "plt.tight_layout(rect=[0,0,1,.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-eIrx_bYm6QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a few random digits\n",
        "fig,axs = plt.subplots(3,4,figsize=(10,6))\n",
        "\n",
        "for ax in axs.flatten():\n",
        "  # pick a random image\n",
        "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
        "\n",
        "  # create the image (must be reshaped!)\n",
        "  img = data[randimg2show,:]\n",
        "  ax.plot(img,'ko')\n",
        "\n",
        "  # title\n",
        "  ax.set_title('The number %i'%labels[randimg2show])\n",
        "\n",
        "plt.suptitle('How computer see the data',fontsize=20)\n",
        "plt.tight_layout(rect=[0,0,1,.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7LZDjI4moM4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see some example 7s\n",
        "\n",
        "# find indices of all the 7's in the dataset\n",
        "the7s = np.where(labels==7)[0]\n",
        "\n",
        "# draw the first 12\n",
        "fig,axs = plt.subplots(2,6,figsize=(15,6))\n",
        "\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "  img = np.reshape(data[the7s[i],:],(28,28))\n",
        "  ax.imshow(img,cmap='gray')\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.suptitle(\"Example 7's\",fontsize=20)\n",
        "plt.tight_layout(rect=[0,0,1,.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jhu7-GFrqkL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how similar are all the 7's? \n",
        "\n",
        "# how many 7's are there?\n",
        "print(data[the7s,:].shape)\n",
        "\n",
        "\n",
        "# let's see how they relate to each other by computing spatial correlations\n",
        "C = np.corrcoef(data[the7s,:])\n",
        "\n",
        "# and visualize\n",
        "fig,ax = plt.subplots(1,3,figsize=(16,6))\n",
        "ax[0].imshow(C,vmin=0,vmax=1)\n",
        "ax[0].set_title(\"Correlation across all 7's\")\n",
        "\n",
        "# extract the unique correlations and show as a scatterplot\n",
        "uniqueCs = np.triu(C,k=1).flatten()\n",
        "ax[1].hist(uniqueCs[uniqueCs!=0],bins=100)\n",
        "ax[1].set_title('All unique correlations')\n",
        "ax[1].set_xlabel(\"Correlations of 7's\")\n",
        "ax[1].set_ylabel('Count')\n",
        "\n",
        "# show all 7's together\n",
        "aveAll7s = np.reshape( np.mean(data[the7s,:],axis=0) ,(28,28))\n",
        "ax[2].imshow(aveAll7s,cmap='gray')\n",
        "ax[2].set_title(\"All 7's averaged together\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jsbXrEtjrO-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wtDpCpS7rPy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "iig9qJoAtS_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dtaa normalization\n",
        "dataNorm = data/np.max(data)"
      ],
      "metadata": {
        "id": "cw5wiQay1j3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# convert to tensor\n",
        "dataT= torch.tensor(dataNorm).float()\n",
        "labelsT = torch.tensor(labels).long()\n",
        "\n",
        "# split the dataset\n",
        "train_data, test_data, train_label, test_label = train_test_split(dataT, labelsT, test_size=0.1, stratify=labelsT)\n",
        "\n",
        "# convert to pytorch dtaasets\n",
        "train_data = TensorDataset(train_data, train_label)\n",
        "test_data = TensorDataset(test_data, test_label)\n",
        "\n",
        "# translate to DataLoader object\n",
        "batch_size=32\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=test_data.tensors[0].shape[0])\n",
        "                          \n"
      ],
      "metadata": {
        "id": "Ak9vlLvC1wmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# check all variables in workspace\n",
        "%whos"
      ],
      "metadata": {
        "id": "1DUXTgPV3jzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet():\n",
        "\n",
        "  class MNISTANN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.input = nn.Linear(784, 64)\n",
        "\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      self.output = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, X):\n",
        "      X = F.relu(self.input(X))\n",
        "      X = F.relu(self.fc1(X))\n",
        "      X = F.relu(self.fc2(X))\n",
        "      ## log-softmax the output, because I'm using NLLLoss instead of CrossEntropyLoss\n",
        "      X = torch.log_softmax(self.output(X), axis=1)\n",
        "\n",
        "      return X\n",
        "\n",
        "  model = MNISTANN()\n",
        "\n",
        "  loss_func = nn.NLLLoss()\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "  return model, loss_func, optimizer\n"
      ],
      "metadata": {
        "id": "Mj-v-eDwtZe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model with one batch\n",
        "net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "X,y = iter(train_dataloader).next()\n",
        "yHat = net(X)\n",
        "\n",
        "# values are log-probability of each number (0-9)\n",
        "# print(torch.exp(yHat))\n",
        "\n",
        "# now let's compute the loss\n",
        "loss = lossfun(yHat,y)\n",
        "print(' ')\n",
        "print('Loss:')\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "3rW1eu011MpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "  num_epochs = 100\n",
        "\n",
        "  #create the model\n",
        "  net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "  # initilize losses\n",
        "  losses = torch.zeros(num_epochs)\n",
        "  train_accuracy = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(num_epochs):\n",
        "    #training mode\n",
        "    net.train()\n",
        "\n",
        "    batch_accuracy = []\n",
        "    batch_loss = []\n",
        "    # loop over training data batches\n",
        "    for X,y in train_dataloader:\n",
        "      yhat = net(X)\n",
        "      loss = lossfun(yhat, y)\n",
        "\n",
        "      #backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_loss.append(loss.item())\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yhat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batch_accuracy.append( accuracyPct )\n",
        "      #end of batch loop\n",
        "      \n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    train_accuracy.append( np.mean(batch_accuracy) )            # add to list of accuracies\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batch_loss)\n",
        "\n",
        "    ### test accuracy\n",
        "\n",
        "    # switching off the training mode\n",
        "    net.eval()\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_dataloader)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yhat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    test_accuracy.append( 100*torch.mean((torch.argmax(yhat,axis=1)==y).float()) )\n",
        "\n",
        "    print(f\"epoch: {epochi}, train_accuracy: {np.mean(batch_accuracy)}, test_accuracy: {100*torch.mean((torch.argmax(yhat,axis=1)==y).float())}\")\n",
        "  \n",
        "\n",
        "  # function output\n",
        "  return train_accuracy,test_accuracy,losses, net\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BynumviD5X42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAcc,testAcc,losses,net = train_model()"
      ],
      "metadata": {
        "id": "Xn3-A4M29xbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim([0,3])\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10,100])\n",
        "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T_2Rfq3O905E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model through for the test data\n",
        "X,y = next(iter(test_dataloader))\n",
        "predictions = net(X).detach()\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "VPOI7ol0AVml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evidence for all numbers from one sample\n",
        "sample2show = 120\n",
        "\n",
        "plt.bar(range(10),predictions[sample2show]) # try adding exp!\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Number')\n",
        "plt.ylabel('Evidence for that number')\n",
        "plt.title('True number was %s' %y[sample2show].item())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UwqQnA6aAtZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evidence for all numbers from one sample\n",
        "sample2show = 120\n",
        "\n",
        "plt.bar(range(10),torch.exp(predictions[sample2show])) # try adding exp!\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Number')\n",
        "plt.ylabel('Evidence for that number')\n",
        "plt.title('True number was %s' %y[sample2show].item())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gP211XZjEiEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the errors\n",
        "errors = np.where( torch.max(predictions,axis=1)[1] != y )[0]\n",
        "print(errors)\n",
        "\n",
        "# Evidence for all numbers from one sample\n",
        "sample2show = 10\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(14,5))\n",
        "\n",
        "ax[0].bar(range(10),np.exp(predictions[errors[sample2show]]))\n",
        "ax[0].set_xticks(range(10))\n",
        "ax[0].set_xlabel('Number')\n",
        "ax[0].set_ylabel('Evidence for that number')\n",
        "ax[0].set_title('True number: %s, model guessed %s' \n",
        "                %( y[errors[sample2show]].item(), torch.argmax(predictions[errors[sample2show]]).item() ))\n",
        "\n",
        "ax[1].imshow( np.reshape(X[errors[sample2show],:],(28,28)) ,cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FpZHiBRRBPQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOES OUR MODEL NEEDS A RANGE OF PIXEL VALUES"
      ],
      "metadata": {
        "id": "Zfw20rwPBcJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')\n"
      ],
      "metadata": {
        "id": "X1R2R96KFqpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]"
      ],
      "metadata": {
        "id": "i7unKoh0F9fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data to 0 or 1\n",
        "dataNorm = (data > 0).astype(float)\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
        "ax[0].hist(data.flatten(),50)\n",
        "ax[0].set_xlabel('Pixel intensity values')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_title('Histogram of original data')\n",
        "\n",
        "ax[1].hist(dataNorm.flatten(),50)\n",
        "ax[1].set_xlabel('Pixel intensity values')\n",
        "ax[1].set_ylabel('Count')\n",
        "ax[1].set_title('Histogram of normalized data')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v3jmFuWPF94S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm that the data have limited values\n",
        "print( np.unique(data) )\n",
        "print('')\n",
        "print( np.unique(dataNorm) )"
      ],
      "metadata": {
        "id": "Oa3FQAYwGA5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: convert to tensor\n",
        "dataT   = torch.tensor( dataNorm ).float()\n",
        "labelsT = torch.tensor( labels ).long()\n",
        "\n",
        "# Step 2: use scikitlearn to split the data\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "id": "Axw71e0DGFt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet():\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      ### input layer\n",
        "      self.input = nn.Linear(784,64)\n",
        "      \n",
        "      ### hidden layer\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      ### output layer\n",
        "      self.output = nn.Linear(32,10)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      x = F.relu( self.fc1(x) )\n",
        "      x = F.relu( self.fc2(x) )\n",
        "      return torch.log_softmax( self.output(x),axis=1 )\n",
        "      # NOTE: log-softmax the output (b/c loss function)\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet()\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.NLLLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "NSaq7slzGKaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def funtion2trainTheModel():\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 60\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "  # initialize losses\n",
        "  losses    = torch.zeros(numepochs)\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "metadata": {
        "id": "C36XDlqrGRSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAcc,testAcc,losses,net = funtion2trainTheModel()"
      ],
      "metadata": {
        "id": "1fQf2E63GRlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim([0,3])\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10,100])\n",
        "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NrUJk0_JGW8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model through for the test data\n",
        "X,y = next(iter(test_loader))\n",
        "predictions = net(X).detach()"
      ],
      "metadata": {
        "id": "WWi6fLcEGZw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evidence for all numbers from one sample\n",
        "sample2show = 120\n",
        "\n",
        "plt.bar(range(10),predictions[sample2show]) # try adding exp!\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Number')\n",
        "plt.ylabel('Evidence for that number')\n",
        "plt.title('True number was %s' %y[sample2show].item())\n",
        "plt.show()\n",
        "# np.exp(predictions[sample2show])"
      ],
      "metadata": {
        "id": "yBnDiCYEGcIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the errors\n",
        "errors = np.where( torch.max(predictions,axis=1)[1] != y )[0]\n",
        "print(errors)\n",
        "\n",
        "# Evidence for all numbers from one sample\n",
        "sample2show = 14\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(14,5))\n",
        "\n",
        "ax[0].bar(range(10),np.exp(predictions[errors[sample2show]]))\n",
        "ax[0].set_xticks(range(10))\n",
        "ax[0].set_xlabel('Number')\n",
        "ax[0].set_ylabel('Evidence for that number')\n",
        "ax[0].set_title('True number: %s, model guessed %s' \n",
        "                %( y[errors[sample2show]].item(), torch.argmax(predictions[errors[sample2show]]).item() ))\n",
        "\n",
        "ax[1].imshow( np.reshape(X[errors[sample2show],:],(28,28)) ,cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nFJqQIaLGeSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eq4wMA2fGgLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code challenge Data Normalization"
      ],
      "metadata": {
        "id": "cYmSEKXkHPlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]"
      ],
      "metadata": {
        "id": "SqFYfuLNHcdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: convert to tensor\n",
        "dataT   = torch.tensor( data ).float()\n",
        "labelsT = torch.tensor( labels ).long()\n",
        "\n",
        "# Step 2: use scikitlearn to split the data\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "\n",
        "# REMINDER of 3 chalenges:\n",
        "# 0) normalize train to [0 1] and test to [0 1]\n",
        "# 1) normalize train to [0 1] and test to [0 255]\n",
        "# 2) normalize train to [0 255] and test to [0 1]\n",
        "# train_data = train_data/torch.max(train_data)\n",
        "test_data  = test_data/torch.max(test_data)\n",
        "\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "id": "Y-NT3BMFH0nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm ranges of train and test data\n",
        "\n",
        "print('Training data range %g to %g' \n",
        "      %(torch.min(train_loader.dataset.tensors[0]),torch.max(train_loader.dataset.tensors[0])) )\n",
        "\n",
        "print('Test data range %g to %g' \n",
        "      %(torch.min(test_loader.dataset.tensors[0]),torch.max(test_loader.dataset.tensors[0])) )"
      ],
      "metadata": {
        "id": "1enbulZCIDZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet():\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      ### input layer\n",
        "      self.input = nn.Linear(784,64)\n",
        "      \n",
        "      ### hidden layer\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      ### output layer\n",
        "      self.output = nn.Linear(32,10)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      x = F.relu( self.fc1(x) )\n",
        "      x = F.relu( self.fc2(x) )\n",
        "      return self.output(x)\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet()\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "ce2VJ2OjIKvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def funtion2trainTheModel():\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 60\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "  # initialize losses\n",
        "  losses    = torch.zeros((numepochs,2))\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi,0] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "    loss = lossfun(yHat,y)\n",
        "    losses[epochi,1] = loss.item()\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "metadata": {
        "id": "fi32BKXeINgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAcc,testAcc,losses,net = funtion2trainTheModel()"
      ],
      "metadata": {
        "id": "tE2WhptbIPXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend(['Train','Test'])\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10,100])\n",
        "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fNbCNB1CISD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a5m7LELxITWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DISTRIBUTION OF WEIGHTS PRE AND POST LEARNING"
      ],
      "metadata": {
        "id": "RkyaZxU9KOZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "va90mZ8LLcCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# normalize the data to a range of [0 1]\n",
        "dataNorm = data / np.max(data)"
      ],
      "metadata": {
        "id": "MnBrgAO0Nran"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: convert to tensor\n",
        "dataT   = torch.tensor( dataNorm ).float()\n",
        "labelsT = torch.tensor( labels ).long()\n",
        "\n",
        "# Step 2: use scikitlearn to split the data\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "id": "jPPqgUAvNtyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet():\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      ### input layer\n",
        "      self.input = nn.Linear(784,64)\n",
        "      \n",
        "      ### hidden layer\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      ### output layer\n",
        "      self.output = nn.Linear(32,10)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      x = F.relu( self.fc1(x) )\n",
        "      x = F.relu( self.fc2(x) )\n",
        "      return self.output(x)\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet()\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "HUyBiX6VNx_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### exploring the \"innards\" of the model\n",
        "\n",
        "# create a temp model to explore\n",
        "net = createTheMNISTNet()[0]\n",
        "\n",
        "# summary of the entire model\n",
        "print('Summary of model:')\n",
        "print(net)\n",
        "print(' ')\n",
        "\n",
        "# # explore one of the layers\n",
        "print('Summary of input layer:')\n",
        "print( vars(net.input) )\n",
        "print(' ')\n",
        "\n",
        "# # check out the matrix of weights\n",
        "print('Input layer weights:')\n",
        "print( net.input.weight.shape )\n",
        "print( net.input.weight )\n",
        "print(' ')\n",
        "\n",
        "# # finally, extract the weights and make a histogram\n",
        "w = net.input.weight.detach().flatten()\n",
        "plt.hist(w,40)\n",
        "plt.xlabel('Weight value')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of initialized input-layer weights')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0VgfwTCON0A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that returns a histogram of all weights (across all layers)\n",
        "\n",
        "def weightsHistogram(net):\n",
        "\n",
        "  # initialize weight vector\n",
        "  W = np.array([])\n",
        "  \n",
        "  # concatenate each set of weights\n",
        "  for layer in net.parameters():\n",
        "    W = np.concatenate((W,layer.detach().flatten().numpy() ))\n",
        "\n",
        "  # compute their histogram (note: range is hard-coded)\n",
        "  histy,histx = np.histogram(W,bins=np.linspace(-.8,.8,101),density=True)\n",
        "  histx = (histx[1:]+histx[:-1])/2\n",
        "  return histx,histy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# test it!\n",
        "histx,histy = weightsHistogram(net)\n",
        "plt.plot(histx,histy);"
      ],
      "metadata": {
        "id": "At3LIz2xN7mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def funtion2trainTheModel():\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 100\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "  # initialize losses and accuracies\n",
        "  losses    = torch.zeros(numepochs)\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "  # initialize histogram variables\n",
        "  histx = np.zeros((numepochs,100))\n",
        "  histy = np.zeros((numepochs,100))\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # get the weights distribution at the start of this epoch\n",
        "    histx,histy[epochi,:] = weightsHistogram(net)\n",
        "  \n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches        = torch.argmax(yHat,axis=1) == y # booleans (false/true)\n",
        "      matchesNumeric = matches.float()                # convert to numbers (0/1)\n",
        "      accuracyPct    = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )                  # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net,histx,histy\n"
      ],
      "metadata": {
        "id": "ijYHgNnvOcpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAcc,testAcc,losses,net,histx,histy = funtion2trainTheModel()\n"
      ],
      "metadata": {
        "id": "urI9Eg87OnjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim([0,3])\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10,100])\n",
        "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ac5PB-v3Opcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the histogram of the weights\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
        "\n",
        "for i in range(histy.shape[0]):\n",
        "  ax[0].plot(histx,histy[i,:],color=[1-i/100,.3,i/100])\n",
        "\n",
        "ax[0].set_title('Histograms of weights')\n",
        "ax[0].set_xlabel('Weight value')\n",
        "ax[0].set_ylabel('Density')\n",
        "\n",
        "\n",
        "ax[1].imshow(histy,vmin=0,vmax=3,\n",
        "             extent=[histx[0],histx[-1],0,99],aspect='auto',origin='lower',cmap='hot')\n",
        "ax[1].set_xlabel('Weight value')\n",
        "ax[1].set_ylabel('Training epoch')\n",
        "ax[1].set_title('Image of weight histograms')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v7HlW8pYOrTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BREADTH VS DEPTH\n",
        "\n",
        "DEPTH : NUMBER OF HIDDEN LAYERS (LAYERS BETWEEN INPUT AND OTPUT)\n",
        "\n",
        "BREADTH: THE NUMBER OF UNITS PER HIDDEN LAYER (CAN VARY ACCROSS LAYERS)"
      ],
      "metadata": {
        "id": "fDVhK7NaOyYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "1uIIEUGHPdXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# normalize the data to a range of [0 1]\n",
        "dataNorm = data / np.max(data)"
      ],
      "metadata": {
        "id": "w237dazBX8Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: convert to tensor\n",
        "dataT   = torch.tensor( dataNorm ).float()\n",
        "labelsT = torch.tensor( labels ).long()\n",
        "\n",
        "# Step 2: use scikitlearn to split the data\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "train_data = torch.utils.data.TensorDataset(train_data,train_labels)\n",
        "test_data  = torch.utils.data.TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "id": "psqUld4yX-Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet(nUnits,nLayers):\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self,nUnits,nLayers):\n",
        "      super().__init__()\n",
        "\n",
        "      # create dictionary to store the layers\n",
        "      self.layers = nn.ModuleDict()\n",
        "      self.nLayers = nLayers\n",
        "\n",
        "      ### input layer\n",
        "      self.layers['input'] = nn.Linear(784,nUnits)\n",
        "      \n",
        "      ### hidden layers\n",
        "      for i in range(nLayers):\n",
        "        self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
        "\n",
        "      ### output layer\n",
        "      self.layers['output'] = nn.Linear(nUnits,10)\n",
        "    \n",
        "\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      # input layer\n",
        "      x = self.layers['input'](x)\n",
        "\n",
        "      # hidden layers\n",
        "      for i in range(self.nLayers):\n",
        "        x = F.relu( self.layers[f'hidden{i}'](x) )\n",
        "      \n",
        "      # return output layer\n",
        "      x = self.layers['output'](x)\n",
        "      return x\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet(nUnits,nLayers)\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "8CIQ1mo5X_cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an instance of the model and confirm that it returns the expected network.\n",
        "nUnitsPerLayer = 12\n",
        "nLayers = 4\n",
        "net = createTheMNISTNet(nUnitsPerLayer,nLayers)\n",
        "net"
      ],
      "metadata": {
        "id": "dINtXlE9dfh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def funtion2trainTheModel(nUnits,nLayers):\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 60\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet(nUnits,nLayers)\n",
        "\n",
        "  # initialize losses\n",
        "  losses    = torch.zeros(numepochs)\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "metadata": {
        "id": "3DWOumAYdhwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### run the experiment! (note: takes ~30 mins)\n",
        "\n",
        "# define the model parameters\n",
        "numlayers = range(1,4)           # number of hidden layers\n",
        "numunits  = np.arange(50,251,50) # units per hidden layer\n",
        "\n",
        "# initialize output matrices\n",
        "accuracies  = np.zeros((2,len(numunits),len(numlayers)))\n",
        "\n",
        "# start the experiment!\n",
        "for unitidx in range(len(numunits)):\n",
        "  for layeridx in range(len(numlayers)):\n",
        "\n",
        "    # create and train a fresh model\n",
        "    trainAcc,testAcc,losses,net = funtion2trainTheModel(numunits[unitidx],numlayers[layeridx])\n",
        "\n",
        "    # store the results (average of final 5 epochs)\n",
        "    accuracies[0,unitidx,layeridx] = np.mean(trainAcc[-5:])\n",
        "    accuracies[1,unitidx,layeridx] = np.mean(testAcc[-5:])\n",
        "\n",
        "    # print a friendly status message\n",
        "    print(f'Finished units {unitidx+1}/{len(numunits)} and layers {layeridx+1}/{len(numlayers)}') \n"
      ],
      "metadata": {
        "id": "eMBurnAodp5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show accuracy as a function of model depth\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,6))\n",
        "\n",
        "ax[0].plot(numunits,accuracies[0,:,:],'o-',markerfacecolor='w',markersize=9)\n",
        "ax[1].plot(numunits,accuracies[1,:,:],'o-',markerfacecolor='w',markersize=9)\n",
        "\n",
        "for i in range(2):\n",
        "  ax[i].legend(numlayers)\n",
        "  ax[i].set_ylabel('Accuracy')\n",
        "  ax[i].set_xlabel('Number of hidden units')\n",
        "  ax[i].set_title([ 'Train' if i==0 else 'Test' ][0])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vfLMRwqZduFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizers_code challenge"
      ],
      "metadata": {
        "id": "ysv4iTRpdvpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet(optimizerAlgo,learningrate):\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      ### input layer\n",
        "      self.input = nn.Linear(784,64)\n",
        "      \n",
        "      ### hidden layer\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      ### output layer\n",
        "      self.output = nn.Linear(32,10)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      x = F.relu( self.fc1(x) )\n",
        "      x = F.relu( self.fc2(x) )\n",
        "      return self.output(x)\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet()\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optifun = getattr( torch.optim,optimizerAlgo )\n",
        "  optimizer = optifun(net.parameters(),lr=learningrate)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "1YbxfHZdjX3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def funtion2trainTheModel(optimizerAlgo,learningrate):\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 100\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet(optimizerAlgo,learningrate)\n",
        "\n",
        "  # initialize losses\n",
        "  losses    = torch.zeros(numepochs)\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "metadata": {
        "id": "gUzvcWwkjaNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: this experiment takes ~30 mins\n",
        "\n",
        "# variables to loop over\n",
        "learningRates = np.logspace(np.log10(.0001),np.log10(.1),6)\n",
        "optimTypes = ['SGD','RMSprop','Adam']\n",
        "\n",
        "# initialize performance matrix\n",
        "finalPerformance = np.zeros((len(learningRates),len(optimTypes)))\n",
        "\n",
        "\n",
        "# now for the experiment!\n",
        "for idx_o,opto in enumerate(optimTypes):\n",
        "  for idx_l,lr in enumerate(learningRates):\n",
        "    trainAcc,testAcc,losses,net = funtion2trainTheModel(opto,lr)\n",
        "    finalPerformance[idx_l,idx_o] = np.mean(testAcc[-10:])"
      ],
      "metadata": {
        "id": "6aXXXXWLjcrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot the results! \n",
        "plt.plot(learningRates,finalPerformance,'o-',linewidth=2)\n",
        "plt.legend(optimTypes)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rates')\n",
        "plt.ylabel('Test accuracy (ave. last 10 epochs)')\n",
        "plt.title('Comparison of optimizers by learning rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hPtcIl42jiFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### experiment on scrambled data"
      ],
      "metadata": {
        "id": "urxAbVI5jwMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# normalize the data to a range of [0 1]\n",
        "dataNorm = data / np.max(data)"
      ],
      "metadata": {
        "id": "b_4bNNsqk5T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly scramble the data,\n",
        "# preserving the re-ordering for each image\n",
        "eggs = np.random.permutation(data.shape[1])\n",
        "scrambled = dataNorm[:,eggs]\n",
        "\n",
        "\n",
        "# show a few random digits\n",
        "fig,axs = plt.subplots(3,4,figsize=(10,6))\n",
        "\n",
        "for ax in axs.flatten():\n",
        "  # pick a random image\n",
        "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
        "\n",
        "  # create the image (must be reshaped!)\n",
        "  img = np.reshape(scrambled[randimg2show,:],(28,28))\n",
        "  ax.imshow(img,cmap='gray')\n",
        "\n",
        "  # title\n",
        "  ax.set_title('The number %i'%labels[randimg2show])\n",
        "\n",
        "plt.suptitle('The scrambled data',fontsize=20)\n",
        "plt.tight_layout(rect=[0,0,1,.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Br3LwyTlBED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: convert to tensor\n",
        "dataT   = torch.tensor( scrambled ).float()\n",
        "labelsT = torch.tensor( labels ).long()\n",
        "\n",
        "# Step 2: use scikitlearn to split the data\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "id": "WomGVQ10lCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet():\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      ### input layer\n",
        "      self.input = nn.Linear(784,64)\n",
        "      \n",
        "      ### hidden layer\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      ### output layer\n",
        "      self.output = nn.Linear(32,10)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      x = F.relu( self.fc1(x) )\n",
        "      x = F.relu( self.fc2(x) )\n",
        "      return self.output(x)\n",
        "      # NOTE: no log-softmax above because of the loss function below\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet()\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "XJzkxh2zlEOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def funtion2trainTheModel():\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 100\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "  # initialize losses\n",
        "  losses    = torch.zeros(numepochs)\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader))\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "metadata": {
        "id": "zhNLnZsYlMdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAcc,testAcc,losses,net = funtion2trainTheModel()\n"
      ],
      "metadata": {
        "id": "MGauPlp6lN1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim([0,3])\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10,100])\n",
        "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SAd2a1zKlPP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### shifted MNIST DATASET"
      ],
      "metadata": {
        "id": "-FUAcyb0lQzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "EPPadbt0roWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# normalize the data to a range of [0 1]\n",
        "dataNorm = data / np.max(data)"
      ],
      "metadata": {
        "id": "ppOKBsgnrtrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: convert to tensor\n",
        "dataT   = torch.tensor( dataNorm ).float()\n",
        "labelsT = torch.tensor( labels ).long()\n",
        "\n",
        "# Step 2: use scikitlearn to split the data\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "id": "smegF9Jqrvd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first let's see how to shift a vectorized image\n",
        "\n",
        "# grab one image data\n",
        "tmp = test_loader.dataset.tensors[0][0,:]\n",
        "tmp = tmp.reshape(28,28) # reshape to 2D image\n",
        "\n",
        "# shift the image (pytorch calls it \"rolling\")\n",
        "tmpS = torch.roll(tmp,8,dims=1)\n",
        "\n",
        "\n",
        "# now show them both\n",
        "fig,ax = plt.subplots(1,2,figsize=(10,6))\n",
        "ax[0].imshow(tmp, cmap='gray')\n",
        "ax[0].set_title('Original')\n",
        "\n",
        "ax[1].imshow(tmpS, cmap='gray')\n",
        "ax[1].set_title('Shifted (rolled)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7nEYrdJi2D1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now repeat for all images in the test set\n",
        "\n",
        "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
        "  \n",
        "  # get the image\n",
        "  img = test_loader.dataset.tensors[0][i,:]\n",
        "  \n",
        "  # reshape and roll by max. 10 pixels\n",
        "  randroll = np.random.randint(-10,11)\n",
        "  img = torch.roll( img.reshape(28,28) ,randroll,dims=1 )\n",
        "\n",
        "  # re-vectorize and put back into the matrix\n",
        "  test_loader.dataset.tensors[0][i,:] = img.reshape(1,-1)\n",
        "\n",
        "\n",
        "# Note: now run the previous cell again to confirm the shifting"
      ],
      "metadata": {
        "id": "dIuDWnU52FoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a class for the model\n",
        "def createTheMNISTNet():\n",
        "\n",
        "  class mnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      ### input layer\n",
        "      self.input = nn.Linear(784,64)\n",
        "      \n",
        "      ### hidden layer\n",
        "      self.fc1 = nn.Linear(64,32)\n",
        "      self.fc2 = nn.Linear(32,32)\n",
        "\n",
        "      ### output layer\n",
        "      self.output = nn.Linear(32,10)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      x = F.relu( self.fc1(x) )\n",
        "      x = F.relu( self.fc2(x) )\n",
        "      return self.output(x)\n",
        "  \n",
        "  # create the model instance\n",
        "  net = mnistNet()\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "metadata": {
        "id": "3zGc-0Ki2RJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def funtion2trainTheModel():\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 50\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheMNISTNet()\n",
        "\n",
        "  # initialize losses\n",
        "  losses    = torch.zeros(numepochs)\n",
        "  trainAcc  = []\n",
        "  testAcc   = []\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader))\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      \n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "metadata": {
        "id": "_ruVCkyU2UK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAcc,testAcc,losses,net = funtion2trainTheModel()\n"
      ],
      "metadata": {
        "id": "hRhdjqTO2YG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim([0,3])\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10,100])\n",
        "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7O3JDYvx2ZtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BHLEC0nl2bSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}